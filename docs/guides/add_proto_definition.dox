/*! \page tutorial_add_proto_definition Compiling new protobuf message definitions

This guide shows how to to compile and install `.proto` files so they can be used in NRPCore experiments.
It can be easily done by using the provided Python script, `nrp_compile_protobuf.py`.
Afterwards the compiled Protobuf message types can be used by gRPC Engines and TFs.

In order for the script to work it is important that all `.proto` files which are to be compiled have a package specifier as described in the <a href="https://developers.google.com/protocol-buffers/docs/proto#packages">protobuf documentation</a>.
This package specifier is used to name compiled libraries and header files and generated classes.
Compiling .proto files without a package specifier leads to a configuration error.

The script takes two arguments:
- `--proto_files_path`: path to the `.proto` files which should be compiled. The current working directory by default
- `--install_dir`: installation directory. By default this is the folder were NRPCore was installed

The script is installed with NRPCore, so it can be directly invoked from the command line. Eg.:

\code{.sh}
nrp_compile_protobuf.py --help
\endcode

will print the script help information.

When executed, it will compile all the `.proto` files found at `proto_files_path` and install the compiled libraries in `install_dir`.
For a description of the compiled libraries see \ref engine_grpc_protobuf_libraries "here".

Afterwards, the new message definitions will be available to exchange data by gRPC Engines through the *Engine.DataPackMessage* message type.
This is the message type used by gRPC Engine servers to send data to gRPC Engine clients. 
It contains a DataPack Id and the data itself, stored in a *data* field of type *Any*. 
*Any* is a type of field which can store any type of message. 
Those message types compiled with the provided Python script can be sent to gRPC Engine servers wrapped in *Engine.DataPackMessage*.

The *Engine.DataPackMessage* can be found in the folder *nrp-core-msgs/protobuf/nrp_proto_defs*.

\section tutorial_using_proto_grpc Using compiled messages in GRPC Engines

As described \ref engine_grpc_protobuf_libraries "here", GRPC Engine configuration has a "ProtobufPackages" parameter which is of type array and contains the list of Protobuf Packages that can be used by the Engine.

\section tutorial_using_proto_python Using compiled messages in TFs

Python bindings are generated for compiled Protobuf messages which allow to use them in TFs.
See \ref datapacks_protobuf "here" for more details.

For example, lets say that `nrp_compile_protobuf.py` is used to compile a `.proto` file with package name `MyPackage` and containing one message definition `MyMessage`.
Then, a Python module with name `mypackage` is generated.
Containing two classes: `MyPackageMyMessage` which wraps a `MyPackage::MyMessage` c++ object; and `MyPackageMyMessageDataPack`, wrapping a `DataPack<MyPackage::MyMessage>` c++ object.
This is all that is required to use the new Protobuf message definitions in TFs.

\code{.py}
from mypackage import MyPackageMyMessageDataPack
d = MyPackageMyMessageDataPack('model', 'engine')
type(d.data) # returns <class 'mypackage.MyPackageMyMessage'>
\endcode

Again, for more details about the Python wrappers generated for Protobuf messages refer to \ref datapacks_protobuf "here".

\section tutorial_add_proto_definition_nested_package_name Nested Package Specifiers

As commented above, the package specifier in .proto files is used to name compiled libraries and header files.
Thus, using the same package specifier in more than one .proto file will lead to one of them overwritting the other.
In case of wishing to have Protobuf message definitions from multiple .proto files under the same namespace, you can use nested package specifiers instead. As in the code snippet below:

\code{.proto}
package SuperPackage.MyPackage;
message MyMessage
{
    uint32 somefield = 1;
}
\endcode

Compiling this code would generate the following components:
- a `superpackage_mypackage.pb.h` header file generated by `protoc`.
- a `libProtoSuperPackageMyPackage.so` library, containing the cpp code generated by `protoc`.
- a `libNRPProtoSuperPackageMyPackageOps.so` linking to the former one and containing Protobuf conversion functions needed by GRPC Engine clients and servers
- a `<superpackage_mypackage>.so` library, also linking to `libProtoSuperPackageMyPackage.so` and containing Protobuf Python bindings to use the compiled msgs in Transceiver Functions

\section tutorial_add_proto_definition_example Example

Now lets see a full example.

First lets put the code below in a `.proto` file. If the extension is not `.proto` it won't be found by the compilation script.

\code{.proto}
syntax = "proto3";

package MyPackage;

/*
 * Message used for testing
 */
message MyMessage
{
    uint32 integer = 1;
    string str     = 2;
}
\endcode

Now lets compile the file by executing the script from the folder were the file is contained:

\code{.sh}
nrp_compile_protobuf.py
\endcode

After the script execution ends, the package is ready to be used with NRPCore.
Lets test it by implementing an experiment in which a TF will send datapacks of type `MyMessage` to an \ref datatransfer_engine which will log them into a file.

We'll first add the experiment configuration file.
Paste the code below into a file, e.g. `simulation_config.json`:

\code{.json}
{
    "SimulationTimeout": 5,
	"EngineConfigs": [
		{
			"EngineType": "datatransfer_grpc_engine",
			"EngineName": "datatransfer_engine",
			"ServerAddress": "localhost:9006",
			"dataDirectory": "data/test",
			"ProtobufPackages": ["MyPackage"],
			"dumps":[
				{"name": "datapack_1", "network": false, "file": true}
			]
		}
	],
    "DataPackProcessingFunctions": [
		{
			"Name": "tf",
			"FileName": "tf.py"
		}
    ]
}
\endcode

The line:

\code{.json}
"ProtobufPackages": ["MyPackage"]
\endcode

allows the DataTransfer Engine to exchange Protobuf messages from `MyPackage`.

The line: 

\code{.json}
{"name": "datapack_1", "network": false, "file": true}
\endcode

declares a DataPack to be logged, which will be send by a TF and will be of type `MyMessage`:

Now lets write the TF.
Paste this code into a `tf.py` file:

\code{.python}
from nrp_core import *
from mypackage import MyPackageMyMessageDataPack


@TransceiverFunction("datatransfer_engine")
def data_transfer():

    my_proto = MyPackageMyMessageDataPack("datapack_1", "datatransfer_engine")
    my_proto.data.integer = 6
    my_proto.data.str = "six"

    return [my_proto]
\endcode

The TF instantiates a DataPack of type `MyPackageMyMessageDataPack`, modifies it, and send it to `datatransfer_engine`, which will log it.

Finally, lets execute the experiment:

\code{.python}
NRPCoreSim -c simulation_config.json
\endcode

After the experiment completes its execution there should be a new file `data/test/<time_stamp>/datapack_1-0.data` with many rows containing the logged data from `datapack_1` datapack. 

*/
